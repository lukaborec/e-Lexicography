{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.language import Language\n",
    "from spacy.lang.en import English\n",
    "import os\n",
    "from os import listdir\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_sentences = []\n",
    "\n",
    "nlp=spacy.load(\"en\")\n",
    "\n",
    "for text in os.listdir(\"python-crawl/Chunks/\"):\n",
    "    if text != \".DS_Store\":\n",
    "        path = \"python-crawl/Chunks/\" + text\n",
    "\n",
    "        file = open(path, \"r\").read()\n",
    "        \n",
    "        file = file.lower()\n",
    "        \n",
    "        doc = nlp(file)\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            annotated_sentences.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_words = [token for sent in annotated_sentences for token in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_allowed = [\"PROPN\", \"NUM\", \"PUNCT\", \"SYM\", \"SPACE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_words  = [token for token in annotated_words if token.pos_ not in not_allowed and token.orth_.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_words = sorted(filter_words, key=lambda x: x.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing duplicates \n",
    "visited = defaultdict(int)\n",
    "sorted_Dict_words = []\n",
    "\n",
    "for word in sorted_words:\n",
    "    if visited[(word.orth_, word.pos_)] == 0:\n",
    "        visited[(word.orth_, word.pos_)] = 1\n",
    "        sorted_Dict_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[abled,\n",
       " abnegation,\n",
       " abnormal,\n",
       " abnormalities,\n",
       " abnormality,\n",
       " abnormally,\n",
       " aboard,\n",
       " aboard,\n",
       " abode,\n",
       " abolish]"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_Dict_words[110:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools for collocations\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "allowed=[\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"X\", \"ADP\"]\n",
    "\n",
    "# helper function for filtering bigrams\n",
    "def contains_bigram(container, bigram):\n",
    "    for added in container:\n",
    "        if added[0].orth_ == bigram[0].orth_ and added[1].orth_ == bigram[1].orth_:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "nltk_bigrams = list(nltk.bigrams(annotated_words))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(tok):\n",
    "    if tok.orth_ in stopwords:\n",
    "        return []\n",
    "    \n",
    "    filtered = []\n",
    "    \n",
    "    for bigram in nltk_bigrams:\n",
    "        if tok.orth_ in (bigram[0].orth_, bigram[1].orth_):\n",
    "            if bigram[0].pos_ in allowed:\n",
    "                filtered.append(bigram[0])\n",
    "            if bigram[1].pos_ in allowed:\n",
    "                filtered.append(bigram[1])\n",
    "\n",
    "    bigram_finder = BigramCollocationFinder.from_words(filtered)\n",
    "    \n",
    "    try:\n",
    "        word_filter = lambda w1, w2: tok.orth_ not in (w1.orth_, w2.orth_)\n",
    "        bigram_finder.apply_ngram_filter(word_filter)\n",
    "        \n",
    "        phrase_filter = lambda w1, w2: (w1.pos_, w2.pos_) not in allowed_phrases[tok.pos_]\n",
    "        bigram_finder.apply_ngram_filter(phrase_filter)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    calc = bigram_finder.nbest(bigram_measures.pmi, 6)\n",
    "    \n",
    "    filtered = []\n",
    "\n",
    "    for bi in calc:\n",
    "        if not contains_bigram(filtered, bi):\n",
    "            filtered.append(bi)\n",
    "         \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently done with 200 entries.\n",
      "Currently done with 400 entries.\n",
      "Currently done with 600 entries.\n",
      "Currently done with 800 entries.\n",
      "Currently done with 1000 entries.\n",
      "Currently done with 1200 entries.\n",
      "Currently done with 1400 entries.\n",
      "Currently done with 1600 entries.\n",
      "Currently done with 1800 entries.\n",
      "Currently done with 2000 entries.\n",
      "Currently done with 2200 entries.\n",
      "Currently done with 2400 entries.\n",
      "Currently done with 2600 entries.\n",
      "Currently done with 2800 entries.\n",
      "Currently done with 3000 entries.\n",
      "Currently done with 3200 entries.\n",
      "Currently done with 3400 entries.\n",
      "Currently done with 3600 entries.\n",
      "Currently done with 3800 entries.\n",
      "Currently done with 4000 entries.\n",
      "Currently done with 4200 entries.\n",
      "Currently done with 4400 entries.\n",
      "Currently done with 4600 entries.\n",
      "Currently done with 4800 entries.\n",
      "Currently done with 5000 entries.\n",
      "Currently done with 5200 entries.\n",
      "Currently done with 5400 entries.\n",
      "Currently done with 5600 entries.\n",
      "Currently done with 5800 entries.\n",
      "Currently done with 6000 entries.\n"
     ]
    }
   ],
   "source": [
    "# data structure for dictionary creation\n",
    "dictionary = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "count = 0\n",
    "\n",
    "for token in sorted_Dict_words:\n",
    "    if len(dictionary[token.lemma_][token.pos_]) == 0:\n",
    "        # lemma found in:\n",
    "        sentence = token.sent.text.capitalize()\n",
    "        \n",
    "        # this is unneeded -> dictionary[token.lemma_][token.pos_][0] = word form\n",
    "        #dictionary[token.lemma_][token.pos_][1] = sentence in which it is found\n",
    "        \n",
    "        dictionary[token.lemma_][token.pos_].append(token.orth_)\n",
    "        dictionary[token.lemma_][token.pos_].append(sentence)\n",
    "            \n",
    "        # dictionary[token.lemma_][token.pos_][2:] = tuple consisting of a tuple and a string - \n",
    "        # tuple = (w1, w2) tuple of words that make up the collocation \n",
    "        # string = example usage of the collocation\n",
    "                  \n",
    "        collocations = get_bigrams(token)\n",
    "        for bigram in collocations:\n",
    "                dictionary[token.lemma_][token.pos_].append(((bigram[0].lemma_, bigram[1].lemma_), bigram[0].sent.text.capitalize()))\n",
    "    \n",
    "    count+=1\n",
    "    if count%200 == 0:\n",
    "        print(\"Currently done with \" + str(count) + \" entries.\")\n",
    "    if count == 6000:\n",
    "        break\n",
    "                                                  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1465588"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating the dictionary\n",
    "\n",
    "root = etree.Element(\"data\")\n",
    "\n",
    "for word in dictionary.items(): #for each lemma\n",
    "    entry = etree.Element(\"entry\") # children: form\n",
    "    form = etree.Element(\"form\", type=\"lemma\") # children: orth\n",
    "    \n",
    "    orth = etree.Element(\"orth\")\n",
    "    orth.text = word[0]\n",
    "    \n",
    "    form.append(orth)\n",
    "\n",
    "    entry.append(form)\n",
    "    \n",
    "    for pos in word[1].items(): #pos[0] = tag, pos[1] = tuple (collocation, example)\n",
    "        gramGrp = etree.Element(\"gramGrp\") # children: pos\n",
    "        \n",
    "        pos_tag = etree.Element(\"pos\")\n",
    "        pos_tag.text = pos[0]\n",
    "        \n",
    "        gramGrp.append(pos_tag)\n",
    "        # feature[1][0] - ignore\n",
    "        \n",
    "        collocation_text = []\n",
    "        example_sentence = pos[1][1]\n",
    "        \n",
    "        # writing collocation\n",
    "        for cols in pos[1][2:]:\n",
    "            collocation_text.append(cols[0])\n",
    "\n",
    "        # writing test senence\n",
    "        if example_sentence == \"\":\n",
    "            example_sentence = col[0]\n",
    "                \n",
    "        cit = etree.Element(\"cit\")\n",
    "        cit.text = example_sentence\n",
    "        \n",
    "        colloc = etree.Element(\"colloc\")\n",
    "        colloc.text = str(collocation_text).strip('[]')\n",
    "\n",
    "        gramGrp.append(cit)   \n",
    "        gramGrp.append(colloc)\n",
    "        break\n",
    "        \n",
    "    entry.append(gramGrp)\n",
    "    root.append(entry)\n",
    "\n",
    "# tree = etree.ElementTree(root)\n",
    "# print(etree.tostring(tree))\n",
    "writing_file = open(\"python-crawl/dict-teilex15-22.xml\", \"w\")\n",
    "writing_file.write(etree.tostring(root,encoding=str, pretty_print=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63830"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_Dict_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('medical', 'diagnostic'),\n",
       "  'Some don’t find out until they are older , when they run into trouble trying to have children and , through medical diagnostics , find out that their sex is more complicated than they expected .')]"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[\"diagnostic\"][\"NOUN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('diagnostic', 'standard'),\n",
       "  'Until 1973 , homosexuality was a psychological disorder justified in the diagnostic and statistical manual of mental disorders ; the current edition , the dsm-5 , still considers transgender people disabled .')]"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[\"diagnostic\"][\"ADJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
