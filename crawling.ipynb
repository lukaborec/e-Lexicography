{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen, HTTPError\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to trick the website into thinking the spider is user-agent\n",
    "\n",
    "def get_url_data(url = \"\"):\n",
    "    try:\n",
    "        request = Request(url, headers = {'User-Agent' :\\\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\"})\n",
    "        response = urlopen(request)\n",
    "        data = response.read().decode(\"utf8\")\n",
    "        return data\n",
    "    except HTTPError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out the bad URLs\n",
    "visited = []\n",
    "\n",
    "with open(\"python-crawl/crawled_urls.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open(\"python-crawl/crawled_urls_filtered.txt\", \"w\") as wr_f:\n",
    "    for line in lines:\n",
    "        temp = line\n",
    "        \n",
    "        if \"atom-feed\" in temp: # de-duplication\n",
    "            continue\n",
    "            \n",
    "        if \"comment_id\" in temp or \"comments_open\" in temp: # forum discussions\n",
    "            continue\n",
    "            \n",
    "        if (temp.startswith(\"https://aeon.co/essays/\") or temp.startswith(\"https://aeon.co/ideas/\") or temp.startswith(\"https://aeon.co/amp/ideas\")or temp.startswith(\"https://aeon.co/amp/essays\")) and (\"-\" in temp):\n",
    "            temp = temp.replace(\"https://aeon.co/amp/ideas/\", \"\")\n",
    "            temp = temp.replace(\"https://aeon.co/amp/essays/\", \"\")\n",
    "            temp = temp.replace(\"https://aeon.co/essays/\", \"\")\n",
    "            temp = temp.replace(\"https://aeon.co/ideas/\", \"\")\n",
    "\n",
    "            if temp not in visited:\n",
    "                wr_f.write(line)\n",
    "                visited.append(temp)\n",
    "\n",
    "# initial - 9600 links\n",
    "# filtered - 1600 links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41666114"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_file = open(\"python-crawl/crawled_urls_filtered.txt\", \"r\")\n",
    "\n",
    "root = etree.Element(\"data\")\n",
    "\n",
    "for url in url_file:\n",
    "        # downloading data\n",
    "        data = get_url_data(url)\n",
    "        \n",
    "        if data != None:\n",
    "            # extracting data from HTML\n",
    "            soup = BeautifulSoup(data, \"html\")\n",
    "            title = soup.title.text\n",
    "            title = title.replace(\" | Aeon Essays\", \"\")\n",
    "            title = title.replace(\" | Aeon Ideas\", \"\")\n",
    "            \n",
    "            article_body = soup.find(\"div\", attrs = {\"class\":\"has-dropcap\"}).text\n",
    "\n",
    "            categories = soup.find(\"div\" , attrs={\"class\":\"topics-banner\"}).text\n",
    "\n",
    "            authors = soup.find(\"p\", attrs={\"class\":\"article__body__author-name vcard\"}).text\n",
    "\n",
    "            date = soup.find(\"time\", attrs={\"class\":\"article__date published\"}).text\n",
    "            date = date.replace(\"\\n\", \"\")\n",
    "            \n",
    "            # creating an XML file\n",
    "            article = etree.Element(\"article\")\n",
    "            \n",
    "            article_title = etree.Element(\"title\") \n",
    "            article_authors = etree.Element(\"authors\")\n",
    "            article_date = etree.Element(\"date\")\n",
    "            article_category = etree.Element(\"category\")\n",
    "            article_text = etree.Element(\"text\")\n",
    "            \n",
    "            article_title.text = title\n",
    "            article_authors.text = authors\n",
    "            article_date.text = date\n",
    "            article_category.text = categories\n",
    "            article_text.text = article_body\n",
    "            \n",
    "            article.append(article_title)\n",
    "            article.append(article_authors)\n",
    "            article.append(article_date)\n",
    "            article.append(article_category)\n",
    "            article.append(article_text)\n",
    "            \n",
    "            root.append(article)\n",
    "\n",
    "writing_file = open(\"python-crawl/Aeon-corpus.xml\", \"w\")\n",
    "writing_file.write(etree.tostring(root,encoding=str, pretty_print=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
